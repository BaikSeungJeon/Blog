# MNIST

지금까지는 정형 데이터였다면, 이번엔 비정형 데이터를 공부해 봅시다. 비정형 데이터의 대표적인 예로는 이미지가 있습니다. 이미지 데이터 셋을 이용한 다중 분류 문제를 해결해 봅시다.

<code>MNIST</code>는 사람이 손 글씨로 쓴 숫자들을 모아놓은 데이터 셋입니다. 

## 1. 모듈 임포트

```py
# 1. 모듈 임포트
import numpy as np # 넘파이
import pandas as pd # 판다스, csv 불러오기 위함
import matplotlib.pyplot as plt # 맷플롯핍, 이미지로 출력해보기 위함
```

## 2. 로우 데이터 로딩

```py
# 2. 로우 데이터 로딩
# 1) 불러오기
df = pd.read_csv('./data/mnist/train.csv')
# display(df, df.shape) # (42000, 785) -> 총 42000장과 785개의 컬럼

# 2) 이미지 데이터라 결측치와 이상치가 없음

# 3) 이미지 데이터
img_data = df.drop('label', axis=1, inplace=False).values # 맨 앞에 label만 날리면 나머지가 픽셀 데이터라, 이미지 데이터만 가질 수 있음
# display(img_data)

# 4) 그림으로 확인
fig = plt.figure()
fig_arr = []

# interpolation은 보간법을 뜻하며, 픽셀들의 축 위치 간격을 보정하여 이미지가 자연스러운 모양으로 보일 수 있게 하는 방법 
# imshow()에서는 16가지 보간법이 있고, 'nearest'는 가장 고해상도인 보간법임

for n in range(10):
    fig_arr.append(fig.add_subplot(2,5,n+1))
    fig_arr[n].imshow(img_data[n].reshape(28,28), cmap='Greys', interpolation='nearest')
    
plt.tight_layout()
plt.show()
```

![](./images/2023-04-04-14-03-35.png)

## 3. 트레이닝 데이터 셋
> 머신 러닝을 해 보자

```py
# 머신 러닝 해 보기
# 3. 트레이닝 데이터 셋
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# 1) 학습 데이터와 테스트 데이터 분리
x_data_train, x_data_test, t_data_train, t_data_test = \
train_test_split(df.drop('label', axis=1, inplace=False).values,
                 df['label'].values,
                 test_size=0.2
                )

# 2) 결측치, 이상치는 없지만 정규화 진행

scaler = MinMaxScaler()
scaler.fit(x_data_train)
x_data_train_norm = scaler.transform(x_data_train)
x_data_test_norm = scaler.transform(x_data_test)

print(x_data_train_norm)
print(x_data_test_norm)

# 3) one-hot 따로 안 할 것
```

## 4. 모델 생성

```py
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.optimizers import Adam

# 모델 생성
model = Sequential()

# 모델 추가
model.add(Flatten(input_shape=(784, )))
model.add(Dense(10, activation='softmax'))

# 모델 설정
model.compile(optimizer=Adam(learning_rate=1e-4), 
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# 모델 학습
model.fit(x_data_train_norm,
          t_data_train,
          epochs=50,
          validation_split=0.2,
          verbose=1,
          batch_size=100) 
```

## 5. 모델 평가

```py
# 모델 평가
print(model.evaluate(x_data_test_norm, t_data_test)) # [0.2995256781578064, 0.9134523868560791]
```

![](./images/2023-04-04-14-41-23.png)